{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I went from 0 to a business oriented neural network in 7 days #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My journey through my first, proof of concept, tiny neural network that translates English “natural” language into an executable SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "\n",
    "1. For the business reader: get you curious about Machine Intelligence and show that the short term benefits are tangible\n",
    "2. For the IT/BI reader: encourage you to build a neural network for your organisation  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where it started ##\n",
    "\n",
    "While learning more and more about machine learning, I finally looked into [TensorFlow](https://www.tensorflow.org), a state of the art [... open source software library for numerical computation using data flow graphs.].  The tutorial on creating an English to French translation algorithm made me think that, maybe I could apply this to another language I am fluent in: SQL.  SQL is arguably the most used language to query relational databases.   \n",
    "Even if you have never seen a simple SQL query, you could certainly understand it.  To get the *address* of a *store* named \"*Downtown Montreal*\", one could write:\n",
    ">SELECT address FROM store WHERE name = \"Downtown Montreal\"\n",
    "\n",
    "Easy, right!  It can get a lot more complicated, of course.  But you see why I, perhaps naively, think translation should be a piece of cake.\n",
    "\n",
    "**Need an intro to Neural Networks?** Look at this short [video](https://www.youtube.com/watch?v=P2HPcj8lRJE), from DeepLearning.TV.  Do not be afraid, learning stuff is a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where I was 7 days ago\n",
    "* I was sitting on an old, but, as I found out recently, not completely forgotten bachelor’s degree in Mathematics\n",
    "* I accumulated 20 years in Information Technology, mainly in the apparel industry\n",
    "* I had about than 6 months of data science oriented experience with Python and R\n",
    "* I had read a couple of papers on neural networks, listened to a few lectures on *the Interwebs* in the past month  \n",
    "\n",
    "So, I had a little basic understanding, but I was nowhere near a neural network expert!  (I am still nowhere near by the way!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1 : On with the tutorial then!\n",
    ">Every computer program starts with copy-paste.  \n",
    "\n",
    "The idea is not go over the tutorial step by step here, but to reassure my Business reader and my IT reader that, although this journey includes some detours, it has a solid destination. \n",
    "\n",
    "###### Which tutorial?\n",
    "* I chose the [Sequence-to-Sequence Models](https://www.tensorflow.org/tutorials/seq2seq/) tutorial, I figured it was the closest to what I was trying to achieve. (See citation above.)\n",
    "\n",
    "###### Installation and first run\n",
    "* Bugs every step of the way, but mainly due to version control over Python, packages and scripts on GitHub.  Frustrating, but it is fairly easy to find support and answers in forums.  This is completely understandable as the project is actively under development.  I would say that the TensorFlow team and community are doing a very job on supporting this tutorial. \n",
    "* Don’t forget that even if the training starts, it might not be bug free yet!  Here is what I got:\n",
    "![](resource/TrainEnFr.png)\n",
    "* For reference, I am doing this on a high end Mac Book Pro and don’t expect to be able to get to any descent English to French translator on similar computer power… I pulled the proverbial plug on the training after 600 iterations (out of the recommended 340K!), just loading the 22M observations dataset takes ages (though one can easily use a small subset for testing)\n",
    "> At first glance, I was looking at around 1700 hours of training time, if I did not run out of memory first…\n",
    "\n",
    "**First goal acheived**; reproducing the neural network locally. Awesome.  Now let's try to customize it to a SQL translation machine.\n",
    "## Day 4 : Now I have a crappy English-to-French translator\n",
    "I don't mind; I am conviced that with more computer power I could get to an descent translation machine.  Now I need to teach this neural net some SQL.\n",
    "\n",
    "###### Creating a dataset\n",
    "My first dataset was only around 20 English sentences with corresponding SQL queries for training, plus 4 or 5 for testing.  \n",
    "I used Excel for this.  \n",
    "![](resource/ExcelData.png)\n",
    "\n",
    "Columns A, B and C have 3 different versions that produces the same SQL statement.  As you can see, I am not aiming for truly natural language, in a business environment, I figure efficiency will be appreciated more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From generic tutorial to personalised model\n",
    "* We get into a different kind of problems at this step, this time more due to lack of documentation.  The good people at TensorFlow give, perfectly understandably, no rodent’s posterior in my fiddling around with the Python scripts.\n",
    "* Remember that this is the fun part.  I can assure you that the code is robust and you won’t have to fully understand it to get it to work, just a few parameter tweaks are required.\n",
    "* Out of Excel as plain text, the format of the file was not compatible.  I wrote (well, mostly copied&pasted, remember?) a function in Python to fix that. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: grey boxes should be ignored by Business readers!\n",
    "import io\n",
    "def convertutf(filename):\n",
    "    with io.open(filename,'r',encoding='utf16') as f:\n",
    "        text = f.read()\n",
    "    # process Unicode text\n",
    "    with io.open(filename,'w',encoding='utf8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6 : ... Now I have a crappy English-to-SQL translator, sweet!\n",
    "I got a result with my own data!  It was completely of the worst kind, but i saw at that point that I could get somewhere interesting with a few more keystrokes on my magic mat grey metal box with an half eaten apple logo on it.\n",
    "\n",
    "1. The model ignores digits by default, but it can convert numbers to text, not ideal, but I will do for now.\n",
    "2. A bigger dataset\n",
    "    + I am creating data inefficently in Excel, so I only got to around 450 observations.  This should do for a proof of concept.\n",
    "    \n",
    "###### Here is a sample of my training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store number of store 1\n",
      "number of store 2\n",
      "name of store 3\n",
      "description of store 4\n",
      "type of store 5\n",
      "status of store 6\n",
      "city of store 7\n",
      "state of store 8\n",
      "province of store 9\n",
      "size of store 10\n",
      "square foot of store 11\n",
      "square footage of store 12\n",
      "footage of store 13\n",
      "rent of store 14\n",
      "monthly rent of store 15\n",
      "yearly rent of store 16\n",
      "opening date of store 17\n",
      "closing date of store 18\n",
      "address of store 19\n",
      "full address of store 20\n",
      "street address of store 21\n",
      "SELECT store_id FROM store WHERE store_id = 1\n",
      "SELECT store_id FROM store WHERE store_id = 2\n",
      "SELECT name FROM store WHERE store_id = 3\n",
      "SELECT name FROM store WHERE store_id = 4\n",
      "SELECT type FROM store WHERE store_id = 5\n",
      "SELECT active FROM store WHERE store_id = 6\n",
      "SELECT city FROM store WHERE store_id = 7\n",
      "SELECT state FROM store WHERE store_id = 8\n",
      "SELECT state FROM store WHERE store_id = 9\n",
      "SELECT sqr_foot FROM store WHERE store_id = 10\n",
      "SELECT sqr_foot FROM store WHERE store_id = 11\n",
      "SELECT sqr_foot FROM store WHERE store_id = 12\n",
      "SELECT sqr_foot FROM store WHERE store_id = 13\n",
      "SELECT rent FROM store WHERE store_id = 14\n",
      "SELECT rent FROM store WHERE store_id = 15\n",
      "SELECT rent*12 FROM store WHERE store_id = 16\n",
      "SELECT open_date FROM store WHERE store_id = 17\n",
      "SELECT close_date FROM store WHERE store_id = 18\n",
      "SELECT address, city, state FROM store WHERE store_id = 19\n",
      "SELECT address, city, state FROM store WHERE store_id = 20\n",
      "SELECT address FROM store WHERE store_id = 21\n"
     ]
    }
   ],
   "source": [
    "with open(\"resource/giga-fren.release2.fixed.en\") as tdata_en:\n",
    "    alldat_en = tdata_en.read()\n",
    "with open(\"resource/giga-fren.release2.fixed.fr\") as tdata_fr:\n",
    "    alldat_fr = tdata_fr.read()\n",
    "print(alldat_en[0:444] + alldat_fr[0:970])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Day 7 : Ok, I hope this will work\n",
    "\n",
    "\n",
    "##### Start the training \n",
    "![](resource/Training.png)\n",
    "I aborted the process after 9000 steps\n",
    "\n",
    "##### Decode\n",
    "\n",
    "![](resource/Decode.png)\n",
    "\n",
    "Every line that start with \">\" is where I enter a new English sentence, below is the translation from the neural network.  Not good with typos and adjectives, but increasing the dataset should fix that.  And of course, when I improvise a new word in a question, it fails.\n",
    "\n",
    "As you can see, it works pretty well considering the nano-size training set.  Not impressive yet, I must admit.  That is my next task.\n",
    "\n",
    "##### Next step\n",
    "I am still struggling with coming up with a clever way to generate a big amount of data.  My goal is to get it into the 10000 observations range with multiple tables, *group by* clause, etc.  I will publish new results about that as soon as my brain lets me.\n",
    "\n",
    "\n",
    "## So, 7 days later\n",
    "\n",
    "From this experiment, here is what I envision in a business environment.\n",
    "\n",
    "Every day at work, small snippets of info need to be looked up:\n",
    "* How many stores do we have in Ontario?\n",
    "* What is the phone extension of Jane Doe?\n",
    "* What is the inventory value of the Fall 2016 collection?\n",
    "\n",
    "All these questions can surely be answered fairly quickly by a report, the intranet, a colleague, etc.  But what if all these could be answered from a single Google-like search box?  This could significantly change the way employees access information.  And since neural networks are based on data, changing or upgrading a major software will have minimal impact on getting this type of information.\n",
    "\n",
    "Furthermore, Neural networks could be built to seamlessly assist in:\n",
    "* Translating product descriptions\n",
    "* Describing products from an image or sketch\n",
    "* Building a knowledge base from the Support Center email history\n",
    "* Clustering stores\n",
    "* Etc.\n",
    "\n",
    "The use cases are countless!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The neural networks tools available today are mature enough to allow organisations to use them, maybe in a humble manner, but certainly with significant business benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "For reference, these are the Python packages I am working with:\n",
    "![](resource/Packages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "January 19, 2017  \n",
    "Simon Laurin\n",
    "\n",
    "Oh! Did I mention speech recognition..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
